\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\hskip -1em.\nobreakspace  {}Explainable Recommendation System}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}\hskip -1em.\nobreakspace  {}Traditional Method}{2}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The user-item matrix can be factorized as two small matrix: user factor matrix $P$ and item factor matrix $Q^T$, thus based on the matrix multiplication, we can complement the raw matrix and recommend.}}{2}{figure.1}}
\newlabel{fig:long}{{1}{2}{The user-item matrix can be factorized as two small matrix: user factor matrix $P$ and item factor matrix $Q^T$, thus based on the matrix multiplication, we can complement the raw matrix and recommend}{figure.1}{}}
\newlabel{fig:onecol}{{1}{2}{The user-item matrix can be factorized as two small matrix: user factor matrix $P$ and item factor matrix $Q^T$, thus based on the matrix multiplication, we can complement the raw matrix and recommend}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The panel shows the graphical model for constrained PMF. The user's preference for items $R_{ij}$ is determined by the factorized vectorized vectors of users and items which are in line with the Gaussian distributions.}}{3}{figure.2}}
\newlabel{fig:long}{{2}{3}{The panel shows the graphical model for constrained PMF. The user's preference for items $R_{ij}$ is determined by the factorized vectorized vectors of users and items which are in line with the Gaussian distributions}{figure.2}{}}
\newlabel{fig:onecol}{{2}{3}{The panel shows the graphical model for constrained PMF. The user's preference for items $R_{ij}$ is determined by the factorized vectorized vectors of users and items which are in line with the Gaussian distributions}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Neural Network}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The network architecture. The $S^u$ is the user sequence, we used previous 4 actions to predict the next 2 steps. $P_u$ is user embedding and $E^{(u,t)}$ is item embedding.}}{4}{figure.6}}
\newlabel{fig:short}{{6}{4}{The network architecture. The $S^u$ is the user sequence, we used previous 4 actions to predict the next 2 steps. $P_u$ is user embedding and $E^{(u,t)}$ is item embedding}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The process of feature selection in GBDT model.}}{4}{figure.3}}
\newlabel{fig:long}{{3}{4}{The process of feature selection in GBDT model}{figure.3}{}}
\newlabel{fig:onecol}{{3}{4}{The process of feature selection in GBDT model}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces TEM framework. Leveraging the GBDT to select the item and user attributes then embedding, meanwhile, combining the user-item ids embedding feed into attention networks, this is because the different item will have the different attributes, and different uses will focus on different aspects and recommend.}}{4}{figure.4}}
\newlabel{fig:long}{{4}{4}{TEM framework. Leveraging the GBDT to select the item and user attributes then embedding, meanwhile, combining the user-item ids embedding feed into attention networks, this is because the different item will have the different attributes, and different uses will focus on different aspects and recommend}{figure.4}{}}
\newlabel{fig:onecol}{{4}{4}{TEM framework. Leveraging the GBDT to select the item and user attributes then embedding, meanwhile, combining the user-item ids embedding feed into attention networks, this is because the different item will have the different attributes, and different uses will focus on different aspects and recommend}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The user memory network and item memory network. $z_i$ is the memory weights or item embeddings.}}{5}{figure.7}}
\newlabel{fig:short}{{7}{5}{The user memory network and item memory network. $z_i$ is the memory weights or item embeddings}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Conditional RBM for explainable. The explainability unit m means the user's neighbors score to this item, and the movie ratings visible unites is the inward scores.}}{5}{figure.5}}
\newlabel{fig:long}{{5}{5}{Conditional RBM for explainable. The explainability unit m means the user's neighbors score to this item, and the movie ratings visible unites is the inward scores}{figure.5}{}}
\newlabel{fig:onecol}{{5}{5}{Conditional RBM for explainable. The explainability unit m means the user's neighbors score to this item, and the movie ratings visible unites is the inward scores}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Probability Graphic Model}{5}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Sentiment-Aspect-Region Model. u, l, c, r, a, s, d, w, U, L, D, N, represent individual user, individual POI, category, topical-region, review, word, set of users, set of POIs, set of reviews, the number of sentences in a review and the number of words in a sentences. Their inner relation can be represented in this graphics mode.}}{5}{figure.8}}
\newlabel{fig:long}{{8}{5}{Sentiment-Aspect-Region Model. u, l, c, r, a, s, d, w, U, L, D, N, represent individual user, individual POI, category, topical-region, review, word, set of users, set of POIs, set of reviews, the number of sentences in a review and the number of words in a sentences. Their inner relation can be represented in this graphics mode}{figure.8}{}}
\newlabel{fig:onecol}{{8}{5}{Sentiment-Aspect-Region Model. u, l, c, r, a, s, d, w, U, L, D, N, represent individual user, individual POI, category, topical-region, review, word, set of users, set of POIs, set of reviews, the number of sentences in a review and the number of words in a sentences. Their inner relation can be represented in this graphics mode}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces LFM graphical model. r, u, v, p, a, q, b represent the rating, latent factor vector for user, latent factor vector for item, the feature of word in user's review, aspect distribution of word which represent semantic information learned from users’ review text, the feature of word in item's review, aspect distribution of word which represent semantic information learned from items’ review text. From the graphics model, we can obtain the connection between different factors.}}{6}{figure.9}}
\newlabel{fig:short}{{9}{6}{LFM graphical model. r, u, v, p, a, q, b represent the rating, latent factor vector for user, latent factor vector for item, the feature of word in user's review, aspect distribution of word which represent semantic information learned from users’ review text, the feature of word in item's review, aspect distribution of word which represent semantic information learned from items’ review text. From the graphics model, we can obtain the connection between different factors}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Matrix Factorization}{6}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Example of overlapping user-item co-clusters, the white squares can belong to a different cluster, the blue and orange rectangular box.}}{6}{figure.10}}
\newlabel{fig:long}{{10}{6}{Example of overlapping user-item co-clusters, the white squares can belong to a different cluster, the blue and orange rectangular box}{figure.10}{}}
\newlabel{fig:onecol}{{10}{6}{Example of overlapping user-item co-clusters, the white squares can belong to a different cluster, the blue and orange rectangular box}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces AMF, the review text can construct two matrices, IAQM and UAPM, which represent item latent factor matrix and user latent factor matrix respectively. After that, we can reconstruct the rating matrix and recommend.}}{7}{figure.11}}
\newlabel{fig:long}{{11}{7}{AMF, the review text can construct two matrices, IAQM and UAPM, which represent item latent factor matrix and user latent factor matrix respectively. After that, we can reconstruct the rating matrix and recommend}{figure.11}{}}
\newlabel{fig:onecol}{{11}{7}{AMF, the review text can construct two matrices, IAQM and UAPM, which represent item latent factor matrix and user latent factor matrix respectively. After that, we can reconstruct the rating matrix and recommend}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of a star rating and a text review of a phone. Feature, opinion, preference is expressed clearly.}}{7}{figure.12}}
\newlabel{fig:long}{{12}{7}{Example of a star rating and a text review of a phone. Feature, opinion, preference is expressed clearly}{figure.12}{}}
\newlabel{fig:onecol}{{12}{7}{Example of a star rating and a text review of a phone. Feature, opinion, preference is expressed clearly}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Joint tensor decomposition scheme. I, F, U, O is an item, feature, user and opinion phrases latent factors.}}{7}{figure.13}}
\newlabel{fig:long}{{13}{7}{Joint tensor decomposition scheme. I, F, U, O is an item, feature, user and opinion phrases latent factors}{figure.13}{}}
\newlabel{fig:onecol}{{13}{7}{Joint tensor decomposition scheme. I, F, U, O is an item, feature, user and opinion phrases latent factors}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces RMSE vs Ranking.}}{7}{figure.15}}
\newlabel{fig:long}{{15}{7}{RMSE vs Ranking}{figure.15}{}}
\newlabel{fig:onecol}{{15}{7}{RMSE vs Ranking}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\hskip -1em.\nobreakspace  {}Graphic Model}{7}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Schematic diagram of the proposed approach for generating an explanation model by training association rules on the output of a black-box matrix factorization recommendation model.}}{8}{figure.14}}
\newlabel{fig:short}{{14}{8}{Schematic diagram of the proposed approach for generating an explanation model by training association rules on the output of a black-box matrix factorization recommendation model}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces AMF, Sample grounding for predicting likes.}}{8}{figure.16}}
\newlabel{fig:long}{{16}{8}{AMF, Sample grounding for predicting likes}{figure.16}{}}
\newlabel{fig:onecol}{{16}{8}{AMF, Sample grounding for predicting likes}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}\hskip -1em.\nobreakspace  {}Explainable Recommendation Methods Summary}{8}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Major Challenge and Open Question}{8}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Tripartite structure and decomposed graphs. u, a, p is user, item and aspect representation.}}{9}{figure.17}}
\newlabel{fig:long}{{17}{9}{Tripartite structure and decomposed graphs. u, a, p is user, item and aspect representation}{figure.17}{}}
\newlabel{fig:onecol}{{17}{9}{Tripartite structure and decomposed graphs. u, a, p is user, item and aspect representation}{figure.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A classification of above explainable recommendation models. About each method, we show two dimensions: the basic method and the auxiliary information.}}{9}{table.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Future Directions and Promissing Tpois}{9}{section.4}}
\bibstyle{ieee_fullname}
\bibdata{egbib}
\bibcite{Alpher01}{1}
\bibcite{Alpher02}{2}
\bibcite{Alpher03}{3}
\bibcite{Alpher04}{4}
\bibcite{Alpher05}{5}
\bibcite{Alpher06}{6}
\bibcite{Alpher07}{7}
\bibcite{Alpher08}{8}
\bibcite{Alpher09}{9}
\bibcite{Alpher10}{10}
\bibcite{Alpher11}{11}
\bibcite{Alpher12}{12}
\bibcite{Alpher13}{13}
\bibcite{Alpher14}{14}
\bibcite{Alpher15}{15}
\bibcite{Alpher16}{16}
\bibcite{Alpher17}{17}
\bibcite{Alpher18}{18}
\bibcite{Alpher19}{19}
\bibcite{Alpher20}{20}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion}{10}{section.5}}
\bibcite{Alpher21}{21}
\bibcite{Alpher22}{22}
\bibcite{Alpher23}{23}
\bibcite{Alpher24}{24}
\bibcite{Alpher25}{25}
\bibcite{Alpher26}{26}
\bibcite{Alpher27}{27}
\bibcite{Alpher28}{28}
\bibcite{Alpher29}{29}
\bibcite{Alpher30}{30}
\bibcite{Alpher31}{31}
\bibcite{Alpher32}{32}
\bibcite{Alpher33}{33}
